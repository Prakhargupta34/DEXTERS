{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Importing the different datasets\n",
    "dataset1 = pd.read_csv('news_false_final.tsv', delimiter = '\\t\\t', quoting = 3)\n",
    "dataset2 = pd.read_csv('news_true_final.tsv', delimiter = '\\t\\t', quoting = 3)\n",
    "dataset3= pd.read_csv('fake_or_real_news.csv').iloc[:,1:4]\n",
    "dataset3 = dataset3[dataset3['Label'].isin(['True','False'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the Datasets\n",
    "dataset=dataset1.append(dataset2, ignore_index=True)\n",
    "dataset=dataset.append(dataset3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexandria Ocasio-Cortez misrepresents ICE’s d...</td>\n",
       "      <td>False</td>\n",
       "      <td>\"ICE is required to fill 34,000 beds with deta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No, it's not correct that 39% of California st...</td>\n",
       "      <td>False</td>\n",
       "      <td>\"39% of All California Students are illegals.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Viral image overstates births to undocumented ...</td>\n",
       "      <td>False</td>\n",
       "      <td>\"More than 66% of ALL births in California are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump off-base in describing GDP growth...</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Watch those GDP numbers. We started off at a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump wrong that Mercedes, BMW import c...</td>\n",
       "      <td>False</td>\n",
       "      <td>\"The European Union … they send us Mercedes, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Label  \\\n",
       "0  Alexandria Ocasio-Cortez misrepresents ICE’s d...  False   \n",
       "1  No, it's not correct that 39% of California st...  False   \n",
       "2  Viral image overstates births to undocumented ...  False   \n",
       "3  Donald Trump off-base in describing GDP growth...  False   \n",
       "4  Donald Trump wrong that Mercedes, BMW import c...  False   \n",
       "\n",
       "                                             Message  \n",
       "0  \"ICE is required to fill 34,000 beds with deta...  \n",
       "1     \"39% of All California Students are illegals.\"  \n",
       "2  \"More than 66% of ALL births in California are...  \n",
       "3  \"Watch those GDP numbers. We started off at a ...  \n",
       "4  \"The European Union … they send us Mercedes, t...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.dropna()\n",
    "dataset=dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "5.0 % Completed\n",
      "10.0 % Completed\n",
      "15.0 % Completed\n",
      "20.0 % Completed\n",
      "25.0 % Completed\n",
      "30.0 % Completed\n",
      "35.0 % Completed\n",
      "40.0 % Completed\n",
      "45.0 % Completed\n",
      "50.0 % Completed\n",
      "55.0 % Completed\n",
      "60.0 % Completed\n",
      "65.0 % Completed\n",
      "70.0 % Completed\n",
      "75.0 % Completed\n",
      "80.0 % Completed\n",
      "85.0 % Completed\n",
      "90.0 % Completed\n",
      "95.0 % Completed\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, len(dataset)):\n",
    "    news = re.sub('[^a-zA-Z]', ' ', dataset['Headline'][i]+dataset['Message'][i])\n",
    "    news = news.lower()\n",
    "    news = news.split()\n",
    "    ps = PorterStemmer()\n",
    "    news = [ps.stem(word) for word in news if not word in set(stopwords.words('english'))]\n",
    "    news = ' '.join(news)\n",
    "    corpus.append(news)\n",
    "    #For viewing progress\n",
    "    if i*100.0/len(dataset) in range(1,101):\n",
    "        print i*100.0/len(dataset),\"% Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10540, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 2000)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding dependent Variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = dataset.iloc[:, 1].values\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "\n",
    "for i in range(0,len(dataset)):\n",
    "    if(y[i]==2):\n",
    "        y[i]=0\n",
    "    if(y[i]==3):\n",
    "        y[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Splitting dataset into training and testing set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results with Logistic Regression \n",
      "[[843 222]\n",
      " [267 776]]\n",
      "FP: 222    FN: 267\n",
      "TP: 776    TN: 843\n",
      "Accuracy: 0.7680265654648957\n",
      "Precision: 0.7775551102204409\n",
      "Recall: 0.7440076701821668\n",
      "F1 Score: 0.7604115629593337\n"
     ]
    }
   ],
   "source": [
    "#With the logistic regression\n",
    "print \"\\nResults with Logistic Regression \"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier=LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print cm\n",
    "\n",
    "TN=cm[0][0]\n",
    "FP=cm[0][1]\n",
    "TP=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "print \"FP:\",FP,\"   FN:\",FN\n",
    "print \"TP:\",TP,\"   TN:\",TN\n",
    "accuracy =(float) (TP + TN) / (TP + TN + FP + FN)\n",
    "print \"Accuracy:\",accuracy\n",
    "precision =(float)(TP) / (TP + FP)\n",
    "print \"Precision:\",precision\n",
    "recall = (float)(TP) / (TP + FN)\n",
    "print \"Recall:\",recall\n",
    "f1_Score =(float) (2 * precision * recall) / (precision + recall)\n",
    "print \"F1 Score:\",f1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results with Naive Bayes Classifier \n",
      "FP: 107    FN: 518\n",
      "TP: 525    TN: 958\n",
      "Accuracy: 0.7035104364326376\n",
      "Precision: 0.8306962025316456\n",
      "Recall: 0.5033557046979866\n",
      "F1 Score: 0.6268656716417911\n"
     ]
    }
   ],
   "source": [
    "#With the Naive Bayes Classifier\n",
    "print \"\\nResults with Naive Bayes Classifier \"\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN=cm[0][0]\n",
    "FP=cm[0][1]\n",
    "TP=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "print \"FP:\",FP,\"   FN:\",FN\n",
    "print \"TP:\",TP,\"   TN:\",TN\n",
    "accuracy =(float) (TP + TN)/ (TP + TN + FP + FN)\n",
    "print \"Accuracy:\",accuracy\n",
    "precision =(float)(TP) / (TP + FP)\n",
    "print \"Precision:\",precision\n",
    "recall = (float)(TP) / (TP + FN)\n",
    "print \"Recall:\",recall\n",
    "f1_Score =(float) (2 * precision * recall) / (precision + recall)\n",
    "print \"F1 Score:\",f1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: 142    FN: 305\n",
      "TP: 738    TN: 923\n",
      "Accuracy: 0.7879506641366224\n",
      "Precision: 0.8386363636363636\n",
      "Recall: 0.7075743048897412\n",
      "F1 Score: 0.7675507020280812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN=cm[0][0]\n",
    "FP=cm[0][1]\n",
    "TP=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "print \"FP:\",FP,\"   FN:\",FN\n",
    "print \"TP:\",TP,\"   TN:\",TN\n",
    "accuracy =(float) (TP + TN)/ (TP + TN + FP + FN)\n",
    "print \"Accuracy:\",accuracy\n",
    "precision =(float)(TP) / (TP + FP)\n",
    "print \"Precision:\",precision\n",
    "recall = (float)(TP) / (TP + FN)\n",
    "print \"Recall:\",recall\n",
    "f1_Score =(float) (2 * precision * recall) / (precision + recall)\n",
    "print \"F1 Score:\",f1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results with SVM Classifier\n",
      "FP: 142    FN: 305\n",
      "TP: 738    TN: 923\n",
      "Accuracy: 0.7879506641366224\n",
      "Precision: 0.8386363636363636\n",
      "Recall: 0.7075743048897412\n",
      "F1 Score: 0.7675507020280812\n"
     ]
    }
   ],
   "source": [
    "#With the SVM Classifier\n",
    "print \"\\nResults with SVM Classifier\"\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='rbf',random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN=cm[0][0]\n",
    "FP=cm[0][1]\n",
    "TP=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "print \"FP:\",FP,\"   FN:\",FN\n",
    "print \"TP:\",TP,\"   TN:\",TN\n",
    "accuracy = (float) (TP + TN) / (TP + TN + FP + FN)\n",
    "print \"Accuracy:\",accuracy\n",
    "precision =(float)(TP) / (TP + FP)\n",
    "print \"Precision:\",precision\n",
    "recall = (float)(TP) / (TP + FN)\n",
    "print \"Recall:\",recall\n",
    "f1_Score =(float) (2 * precision * recall) / (precision + recall)\n",
    "print \"F1 Score:\",f1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
